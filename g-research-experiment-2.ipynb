{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Loading the libraries and data\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datetime import datetime\nimport gresearch_crypto\nimport datatable as dt\nfrom lightgbm import LGBMRegressor\nimport os\n\n\ndata_folder = \"../input/g-research-crypto-forecasting/\"\n!ls $data_folder","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-02T17:57:12.517410Z","iopub.execute_input":"2022-02-02T17:57:12.517792Z","iopub.status.idle":"2022-02-02T17:57:15.320738Z","shell.execute_reply.started":"2022-02-02T17:57:12.517709Z","shell.execute_reply":"2022-02-02T17:57:15.319660Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Feature Design: \n\nDeisgn relevent features to input into our model. \n\nThe biggest challenge is to input stationary features that can then predict log returns in a highly non stationary enviroment such as the crypto market. ","metadata":{}},{"cell_type":"code","source":"# feature design ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Next steps \n\n- load the dfs. config the data. \n\n- training the model \"choose either one asset or total aset):\n     - reduce memory \n     - load the training set\n     - test the data. \n\n## Feature Engineering: \n     - generate sets of stock price features \n     - no leak/ stationary features \n     - identify groups of highly correlated features and only keep one of them so that you model can have as much predictive power using as few features as possible. \n     - In this context, feature engineering asks: what is the best representation of the sample data to learn a solution to your problem?\n     - some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.\n     - The traditional idea of “Transforming Data” from a raw state to a state suitable for modeling is where feature engineering fits in. Transform data and feature engineering may in fact be synonyms.\n\n- train and selec the validation period. \n\n- Modelling: LightGBM (full model or individual model). \n\n- obtain validation score \n\n- submission \n\n- predict and submit. (before submission, the button has to be active after commit, the following conditions must be met. 9 hours run time.) file name must be named submission.csv \n\nthings to notedown regarding correlation: \n\n- the high but variable correlation between assets. we can see that there is some changing dynamics over time and this would be critical for this time series challenge, in that how to perform forecasts in a highly non stationary enviroment. \n\n- a stationary behaviour of a system or process is characterised by non changing statiscal properties over time such as the mean, variance and autocorrelation. On the other hand, a non stationary behaviour is characterized by a continious change of statiscal proeprties over time. \n\n- Stationarity is important because many useful analytical tools and statistical tests and models rely on it. \n\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FEATURE SELECTION: \n​\n- feature engineering is the process of extracting useful features from raw data using meth, statistics and domain knowledge! \n​\n- Feature selection is primarily focused on removing non-informative or redundant predictors from the model.\n​\n​\n## dimentionality reduction techniques: \n​\n- percent missing values: drop variables that have a very high % of missing values \n​\n- amount of variation: drop or review variables that have a very low variation. standarise variables or use sd to account for variables with difference scales/ drop variables with zero variation. \n​\n- pairwise correlation: many variables are often correlated with each other, hence are redundant. if two variables are highly correlated, keeping only one will help reduce dimentionality without much loss of information\n          - keep the variable that has a higher correlation coefficient with the target. \n​\n- multicolinearity\n- principal component analysis \n- cluster analysis \n​\n- correlation with target: drop variables that have a very low correlation with the target. if a variable has a very low correlation with the target, its not going to be useful for the model (prediction). \n​\n- forward selection: identify the best variables (based on model accuracy), add the next best variables into the model. and so on until some predefined criteria is satisfied. \n​\n- backward elimination RFE: start with all variables included in the model, drop the lest useful variable (eg. based on the smallest drop in model accuracy). and so on until predefined criteria is satisfied. \n​\n- stepwise selection: similar to forward selection process, but a variables can also be dropped if its deemed as not useful any more after a certain number of steps. \n​\n- LASSO (Least Absolute Shrinkage and Seelection Operator): two bird, one stop: variable selection + regularization. \nUsed for regularized regression or logistic regression model. \n​\n- Tree based selection: forests of trees to evaluate the importance of features. Fit a number of randomized decision trees on various sub samples of the dataset and use averaging to rank order features.... \"ensemble of trees\" \n​\n​\n​\ntypes of encoding: \n- nominal encoding. (one hot encoding/ one hot encoding with many categorical/ mean encoding). \n​\n- ordinal encoding..... (label encoding/ target guided ordinal encoding). \n​\n​\n​\n## types of cross validation: \n​\n- leave one out CV..... time intensive, you have to leave keep one data, test all the data out and follow through for every datapoint. \n   - low bias: accuracy goes down, error rate goes up. \n   \n - K fold CV: \n    - example: 1000 data points, k=5, therefore 1000/5= 200. you will test 200 data points and leave the rest. this will be called accuracy 1. And you continue this experiment to see which accuracy n will be the most significant! \n    \n    - you can take the mean/ sd for all accuracy \n    \n    disadvantages: \n      - can have imbalances in datasets hence will not have an           accurate representation of results\n​\n​\n- Satisfied CV: \n​\n- Timeseries CV: \n​","metadata":{}}]}
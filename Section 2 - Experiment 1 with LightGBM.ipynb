{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Experiment 1: LightGBM Algorithm with training data. \n\nFrom data exploration section, it is very evident that we are dealing with a large data set (2.4 million data points). Hence it is crucial to work with a model that can process data at a fast and efficient rate. \n\nLightGBM uses leaf wise (best fit) tree growth. It chooses the lead that minimizes the loss, allowing a growth of an imbalanced tree. Because it doesnt grow level wise, but leaf wise, over fitting can occur if tree depth is not controlled. \n\nThe great attributes about LightGBM is its high speed, it can handle large size of data, takes lower memory to run and most importantly it focuses on accuracy of results.\n\nNow although its attributes, LightGBM is sensitive to overfitting and can easily overfit the data if the parameters are not fine tuned correctly. The high accuracy comes at a cost of high bias in the training data. \n\nThe biggest challenge is going to be parameter tuning and the values I provide to parameters. \n\nNow lets dive in\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"# Loading the libraries and data\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datetime import datetime\nimport gresearch_crypto\nimport datatable as dt\nfrom lightgbm import LGBMRegressor\nimport os\n\n\ndata_folder = \"../input/g-research-crypto-forecasting/\"\n!ls $data_folder","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-31T19:47:14.778144Z","iopub.execute_input":"2022-01-31T19:47:14.778696Z","iopub.status.idle":"2022-01-31T19:47:18.382594Z","shell.execute_reply.started":"2022-01-31T19:47:14.778597Z","shell.execute_reply":"2022-01-31T19:47:18.381575Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"asset_df = dt.fread(\"../input/g-research-crypto-forecasting/asset_details.csv\").to_pandas()\ntrain_df = dt.fread(\"../input/g-research-crypto-forecasting/train.csv\").to_pandas()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:47:18.384977Z","iopub.execute_input":"2022-01-31T19:47:18.385719Z","iopub.status.idle":"2022-01-31T19:48:34.389243Z","shell.execute_reply.started":"2022-01-31T19:47:18.385669Z","shell.execute_reply":"2022-01-31T19:48:34.388502Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"asset_df","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:48:34.390420Z","iopub.execute_input":"2022-01-31T19:48:34.392349Z","iopub.status.idle":"2022-01-31T19:48:34.414486Z","shell.execute_reply.started":"2022-01-31T19:48:34.392317Z","shell.execute_reply":"2022-01-31T19:48:34.413627Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Data Structure for train_df\n\nTrain_df - Training dataset \n\nThe datapoints are as follows:\n\n1. timestamp - A timestamp for the minute covered by the row.\n\n2. Asset_ID - An ID code for the cryptoasset.\n\n3. Count - The number of trades that took place this minute.\n\n4. Open - The USD price at the beginning of the minute.\n\n5. High - The highest USD price during the minute.\n\n6. Low - The lowest USD price during the minute.\n\n7. Close - The USD price at the end of the minute.\n\n8. Volume - The number of cryptoasset u units traded during the minute.\n\n8. VWAP - The volume-weighted average price for the minute.\n\n10. Target - 15 minute residualized returns. See the 'Prediction and Evaluation section of this notebook for details of how the target is calculated.\n\n11. Weight - Weight, defined by the competition hosts here.\n\n12. Asset_Name - Human readable Asset name.","metadata":{}},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:48:34.415941Z","iopub.execute_input":"2022-01-31T19:48:34.416279Z","iopub.status.idle":"2022-01-31T19:48:34.435366Z","shell.execute_reply.started":"2022-01-31T19:48:34.416240Z","shell.execute_reply":"2022-01-31T19:48:34.434583Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING\n\n## Feature Extraction \n\nThe three features I chose for the training model are as follows: \n\n- hlco_ratio: the ratio between High/ Low and Open/Close.\n\n- upper_shadow: Bullish candlestick pattern. \n\n- lower_shadow: Bearish candlestick pattern.\n","metadata":{}},{"cell_type":"code","source":"# Feature Extraction \n\ndef hlco_ratio(df): return (df['High'] - df['Low'])/(df['Close']-df['Open'])\ndef upper_shadow(df): return df['High'] - np.maximum(df['Close'], df['Open'])\ndef lower_shadow(df): return np.minimum(df['Close'], df['Open']) - df['Low']\n\n# A utility function to build features from the original df\n\ndef get_features(df):\n    df_feat = df[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']].copy()\n    df_feat['Upper_Shadow'] = upper_shadow(df_feat)\n    df_feat['Lower_Shadow'] = lower_shadow(df_feat)\n    df_feat['hlco_ration'] = hlco_ratio(df_feat)\n    return df_feat\n","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:48:34.437753Z","iopub.execute_input":"2022-01-31T19:48:34.438263Z","iopub.status.idle":"2022-01-31T19:48:34.447522Z","shell.execute_reply.started":"2022-01-31T19:48:34.438221Z","shell.execute_reply":"2022-01-31T19:48:34.446664Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Main Training Function \n\ndef get_Xy_and_model_for_asset(df_train, asset_id):\n    df = df_train[df_train[\"Asset_ID\"] == asset_id]\n    \n    # TDFH\n    df_proc = get_features(df)\n    df_proc['y'] = df['Target']\n    df_proc = df_proc.dropna(how=\"any\")\n    \n    X = df_proc.drop(\"y\", axis=1)\n    y = df_proc[\"y\"]    \n    model = LGBMRegressor(device = 'gpu')\n    model.fit(X, y)\n    return X, y, model\n","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:48:34.449241Z","iopub.execute_input":"2022-01-31T19:48:34.449755Z","iopub.status.idle":"2022-01-31T19:48:34.457439Z","shell.execute_reply.started":"2022-01-31T19:48:34.449716Z","shell.execute_reply":"2022-01-31T19:48:34.456433Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Loop all over assets\n\nXs = {}\nys = {}\nmodels = {}\n\nfor asset_id, asset_name in zip(asset_df['Asset_ID'], asset_df['Asset_Name']):\n    print(f\"Training model for {asset_name:<16} (ID={asset_id:<2})\")\n    try:\n        X, y, model = get_Xy_and_model_for_asset(train_df, asset_id)    \n        Xs[asset_id], ys[asset_id], models[asset_id] = X, y, model\n    except:         \n        Xs[asset_id], ys[asset_id], models[asset_id] = None, None, None    \n","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:48:34.458886Z","iopub.execute_input":"2022-01-31T19:48:34.459183Z","iopub.status.idle":"2022-01-31T19:49:38.515793Z","shell.execute_reply.started":"2022-01-31T19:48:34.459147Z","shell.execute_reply":"2022-01-31T19:49:38.515146Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Check the model interface\nx = get_features(train_df.iloc[1])\ny_pred = models[0].predict(pd.DataFrame([x]))\ny_pred[0]","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:49:38.519321Z","iopub.execute_input":"2022-01-31T19:49:38.519884Z","iopub.status.idle":"2022-01-31T19:49:38.538147Z","shell.execute_reply.started":"2022-01-31T19:49:38.519847Z","shell.execute_reply":"2022-01-31T19:49:38.537631Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"This is the first experiment, as we can see the prediction figure is extremley low. This could mean one of few things, firstly being that the model has a high error rate. This could be due to the parameter tuning in the main training function. \n\nThe objective is to keep fine tunning the parameters chosen until an accurate result is yielded.\n\nThis is still in progress and is not finished, I will continue uploading more experiments. Stay tuned. ","metadata":{}}]}